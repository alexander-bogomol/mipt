{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar 01: Dealing with Word Embeddings\n",
    "\n",
    "Today we gonna play with word embeddings: train our own little embedding, load one from   gensim model zoo and use it to visualize text corpora.\n",
    "\n",
    "This whole thing is gonna happen on top of embedding dataset.\n",
    "\n",
    "__Requirements:__ if you're running locally, in the selected environment run the following command:\n",
    "\n",
    "```pip install --upgrade nltk gensim bokeh umap-learn```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.6.4)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bokeh\n",
      "  Downloading bokeh-2.4.2-py3-none-any.whl (18.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.5 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting umap-learn\n",
      "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 59.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk) (2021.10.8)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.21.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.8/site-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /opt/conda/lib/python3.8/site-packages (from bokeh) (3.0.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /opt/conda/lib/python3.8/site-packages (from bokeh) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.8/site-packages (from bokeh) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.8/site-packages (from bokeh) (21.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.8/site-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.8/site-packages (from umap-learn) (1.0)\n",
      "Requirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.8/site-packages (from umap-learn) (0.52.0)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 51.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /opt/conda/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (0.35.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (58.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=16.8->bokeh) (2.4.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (3.0.0)\n",
      "Building wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82709 sha256=e1fcac2ea711e2e8d7a6f45cc747398a4d9ffa536fc8e3c231fbdecd8f6a5229\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hj_2wrr4/wheels/f2/64/75/df601da9514261c8cb0830b9515d2b94b5a51f09ddeae92b9e\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53945 sha256=9df42bbcce71429028e5e856ee15d1680a93241498b1d5fe0c588454a31db512\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hj_2wrr4/wheels/1d/07/6e/9ae4e883392994fd1d7c61a0377f0177e3f8e2faff6c677341\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: pynndescent, umap-learn, nltk, gensim, bokeh\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6.4\n",
      "    Uninstalling nltk-3.6.4:\n",
      "      Successfully uninstalled nltk-3.6.4\n",
      "Successfully installed bokeh-2.4.2 gensim-4.1.2 nltk-3.7 pynndescent-0.5.6 umap-learn-0.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk gensim bokeh umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenization:__ a typical first step for an nlp task is to split raw data into words.\n",
    "The text we're working with is in raw format: with all the punctuation and smiles attached to some words, so a simple str.split won't do.\n",
    "\n",
    "Let's use __`nltk`__ - a library that handles many nlp tasks like tokenization, stemming or part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can',\n",
       "  'i',\n",
       "  'get',\n",
       "  'back',\n",
       "  'with',\n",
       "  'my',\n",
       "  'ex',\n",
       "  'even',\n",
       "  'though',\n",
       "  'she',\n",
       "  'is',\n",
       "  'pregnant',\n",
       "  'with',\n",
       "  'another',\n",
       "  'guy',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'baby',\n",
       "  '?'],\n",
       " ['what',\n",
       "  'are',\n",
       "  'some',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'overcome',\n",
       "  'a',\n",
       "  'fast',\n",
       "  'food',\n",
       "  'addiction',\n",
       "  '?']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: lowercase everything and extract tokens with tokenizer. \n",
    "# data_tok should be a list of lists of tokens for each line in data.\n",
    "\n",
    "data_tok = [tokenizer.tokenize(i.lower()) for i in data]\n",
    "data_tok[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can I get back with my ex even though she is pregnant with another guy's baby?\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"can i get back with my ex even though she is pregnant with another guy ' s baby ?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data_tok[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small check that everything is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word vectors:__ as the saying goes, there's more than one way to train word embeddings. There's Word2Vec and GloVe with different objective functions. Then there's fasttext that uses character-level models to train word embeddings. \n",
    "\n",
    "The choice is huge, so let's start someplace small: __gensim__ is another NLP library that features many vector-based models incuding word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data_tok, \n",
    "                 vector_size=32,      # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5).wv  # define context as a 5-word window around the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.074608  , -1.3631552 ,  0.67563766,  3.9778948 ,  0.7398244 ,\n",
       "        1.2136168 ,  2.8465815 , -2.1625822 ,  0.2614606 ,  2.370818  ,\n",
       "        0.25743303,  1.4107958 ,  4.933535  ,  0.5202974 ,  1.8453054 ,\n",
       "       -0.9181827 , -0.02683618, -0.5834884 ,  0.72780555, -4.504232  ,\n",
       "       -4.9351835 , -1.9959066 , -1.4319657 , -0.7701882 ,  0.41352028,\n",
       "       -1.8910207 ,  0.15957822, -0.41841415,  0.10249531,  0.9309142 ,\n",
       "       -0.34934428,  0.02616692], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can get word vectors !\n",
    "model.get_vector('anything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rice', 0.9487574696540833),\n",
       " ('sauce', 0.9302091002464294),\n",
       " ('vodka', 0.9259306788444519),\n",
       " ('butter', 0.9240707755088806),\n",
       " ('potato', 0.9226353764533997),\n",
       " ('cheese', 0.9223295450210571),\n",
       " ('beans', 0.9064363837242126),\n",
       " ('fruit', 0.9002739191055298),\n",
       " ('banana', 0.8997422456741333),\n",
       " ('salad', 0.8993561863899231)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or query similar words directly. Go play with it!\n",
    "model.most_similar('bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained model\n",
    "\n",
    "Took it a while, huh? Now imagine training life-sized (100~300D) word embeddings on gigabytes of text: wikipedia articles or twitter posts. \n",
    "\n",
    "Thankfully, nowadays you can get a pre-trained word embedding model in 2 lines of code (no sms required, promise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tablet', 0.8643448948860168),\n",
       " ('nokia', 0.8390064835548401),\n",
       " ('hp', 0.8380662798881531),\n",
       " ('pc', 0.8283024430274963),\n",
       " ('netbook', 0.819191575050354),\n",
       " ('modem', 0.8145596981048584),\n",
       " ('blackberry', 0.8000472187995911),\n",
       " ('tweetdeck', 0.7989785075187683),\n",
       " ('smartphone', 0.7981544733047485),\n",
       " ('bluetooth', 0.7871382832527161)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"notebook\", \"laptop\"], negative=[\"clock\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing word vectors\n",
    "\n",
    "One way to see if our vectors are any good is to plot them. Thing is, those vectors are in 30D+ space and we humans are more used to 2-3D.\n",
    "\n",
    "Luckily, we machine learners know about __dimensionality reduction__ methods.\n",
    "\n",
    "Let's use that to plot 1000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3154 ,  0.01605, -0.25464,  0.19656,  0.40621, -0.2143 ,\n",
       "        0.88449, -0.40434, -1.2489 ,  0.36182, -0.56618,  1.1953 ,\n",
       "       -4.1326 , -0.28547,  0.15874,  1.0733 ,  1.2445 ,  1.1519 ,\n",
       "       -0.28804,  0.41741, -0.15655,  0.40259, -0.44434,  0.45828,\n",
       "        0.18119], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user>', '_', 'please', 'apa', 'justin', 'text', 'hari', 'playing', 'once', 'sei']\n"
     ]
    }
   ],
   "source": [
    "words = sorted(model.key_to_index.keys(), \n",
    "               key=lambda word: model.key_to_index[word])[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, compute it's vector with model\n",
    "word_vectors = np.array([model.get_vector(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), 25)\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 25)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear projection: PCA\n",
    "\n",
    "The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
    "\n",
    "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n",
    "\n",
    "\n",
    "Under the hood, it attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
    "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pca = PCA(2)\n",
    "scaler = StandardScaler()\n",
    "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
    "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
    "word_vectors_pca = # YOUR CODE\n",
    "# and maybe MORE OF YOUR CODE here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
    "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
    "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's draw it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
    "\n",
    "# hover a mouse over there and see if you can identify the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing neighbors with UMAP\n",
    "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
    "\n",
    "If we instead want to focus on keeping neighboring points near, we could use UMAP, which is itself an embedding method. Here you can read __[more on UMAP (ru)](https://habr.com/ru/company/newprolab/blog/350584/)__ and on __[t-SNE](https://distill.pub/2016/misread-tsne/)__, which is also an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_neighbors=5).fit_transform(word_vectors) # преобразовываем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(embedding[:, 0], embedding[:, 1], token=words)\n",
    "\n",
    "# hover a mouse over there and see if you can identify the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing phrases\n",
    "\n",
    "Word embeddings can also be used to represent short phrases. The simplest way is to take __an average__ of vectors for all tokens in the phrase with some weights.\n",
    "\n",
    "This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.\n",
    "\n",
    "Let's try this new hammer on our data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase):\n",
    "    \"\"\"\n",
    "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
    "    \"\"\"\n",
    "    # 1. lowercase phrase\n",
    "    # 2. tokenize phrase\n",
    "    # 3. average word vectors for all words in tokenized phrase\n",
    "    # skip words that are not in model's vocabulary\n",
    "    # if all words are missing from vocabulary, return zeros\n",
    "    \n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    phrase_tokenized = # YOUR CODE HERE\n",
    "    phrase_vectors = [model[x] for x in phrase_tokenized if x in model.vocab.keys()]\n",
    "\n",
    "    if len(phrase_vectors) != 0:\n",
    "        vector = np.mean(phrase_vectors, axis=0)\n",
    "\n",
    "    # YOUR CODE\n",
    "    \n",
    "    return vector\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_phrase_embedding(data[402687])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's only consider ~5k phrases for a first run.\n",
    "chosen_phrases = data[::len(data) // 1000]\n",
    "\n",
    "# compute vectors for chosen phrases and turn them to numpy array\n",
    "phrase_vectors = np.asarray([get_phrase_embedding(x) for x in chosen_phrases]) # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
    "assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map vectors into 2d space with pca, tsne or your other method of choice\n",
    "# don't forget to normalize\n",
    "\n",
    "phrase_vectors_2d = umap.UMAP(n_neighbors=3).fit_transform(phrase_vectors) # преобразовываем\n",
    "\n",
    "# phrase_vectors_2d = (phrase_vectors_2d - phrase_vectors_2d.mean(axis=0)) / phrase_vectors_2d.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(phrase_vectors_2d[:, 0], phrase_vectors_2d[:, 1],\n",
    "             phrase=[phrase[:50] for phrase in chosen_phrases],\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute vector embedding for all lines in data\n",
    "data_vectors = np.vstack([get_phrase_embedding(l) for l in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(data_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printable_set = set(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = [x for x in data if set(x).issubset(printable_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(query, k=10):\n",
    "    \"\"\"\n",
    "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
    "    similarity should be measured as cosine between query and line embedding vectors\n",
    "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "    query_vector = get_phrase_embedding(query)\n",
    "    dists = data_vectors.dot(query_vector[:, None])[:, 0] / ((norms+1e-16)*np.linalg.norm(query_vector))\n",
    "    nearest_elements = dists.argsort(axis=0)[-k:][::-1]\n",
    "    out = [data[i] for i in nearest_elements]\n",
    "    return out# <YOUR CODE: top-k lines starting from most similar>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
    "\n",
    "print(''.join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[0] == 'How do I get to the dark web?\\n'\n",
    "# assert results[3] == 'What can I do to save the world?\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(query=\"How does Trump?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(query=\"Why don't i ask a question myself?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(np.asarray(phrase_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_colors = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(phrase_vectors_2d[:, 0], phrase_vectors_2d[:, 1], color=[_colors[l] for l in labels],\n",
    "             phrase=[phrase[:50] for phrase in chosen_phrases],\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(phrase_vectors_2d[:,0], phrase_vectors_2d[:, 1], c=labels.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now what?__\n",
    "* Try running TSNE instead of UMAP (it takes a long time)\n",
    "* Try running UMAP or TSNEon all data, not just 1000 phrases\n",
    "* See what other embeddings are there in the model zoo: `gensim.downloader.info()`\n",
    "* Take a look at [FastText](https://github.com/facebookresearch/fastText) embeddings\n",
    "* Optimize find_nearest with locality-sensitive hashing: use [nearpy](https://github.com/pixelogik/NearPy) or `sklearn.neighbors`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: your own word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.autograd  as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(data_tok))\n",
    "\n",
    "word_to_index = # YOUR CODE HERE\n",
    "index_to_word = # YOUR CODE HERE\n",
    "word_counter = {word: 0 for word in word_to_index.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating context pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tuple_list = []\n",
    "w = 4\n",
    "\n",
    "for text in data_tok:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word_to_index[word], word_to_index[text[j]]))\n",
    "                word_counter[word] += 1.\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting everything to `torch.LongTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_torch = torch.tensor(context_tuple_list).type(torch.LongTensor)\n",
    "X_torch = data_torch[:, 0]\n",
    "y_torch = data_torch[:, 1]\n",
    "del data_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2VecModel(25, len(word_to_index)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# To reduce learning rate on plateau of the loss functions\n",
    "lr_scheduler = ReduceLROnPlateau(opt, patience=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(model(X_torch[:5].to(device)), y_torch[:5].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "n_iterations = 1000\n",
    "local_train_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_process(train_loss):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15, 5))\n",
    "\n",
    "    axes.set_title('Loss')\n",
    "    axes.plot(train_loss, label='train')\n",
    "    axes.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iterations):\n",
    "\n",
    "    ix = np.random.randint(0, len(context_tuple_list), batch_size)\n",
    "    x_batch = X_torch[ix].to(device)\n",
    "    y_batch = y_torch[ix].to(device)\n",
    "\n",
    "    # predict log-probabilities or logits\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # compute loss, just like before\n",
    "    ### YOUR CODE\n",
    "\n",
    "\n",
    "    # compute gradients\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # Adam step\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # clear gradients\n",
    "    ### YOUR CODE\n",
    "\n",
    "    local_train_loss_history.append(loss.item())\n",
    "    lr_scheduler.step(local_train_loss_history[-1])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plot_train_process(local_train_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = next(model.embeddings.parameters()).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(word, top_n):\n",
    "    global matrix, word_to_index, index_to_word\n",
    "    y = matrix[word_to_index[word]][None, :]\n",
    "\n",
    "    dist = F.cosine_similarity(matrix,y)\n",
    "    index_sorted = torch.argsort(dist)\n",
    "    top_n = index_sorted[-top_n:]\n",
    "    return [index_to_word[x] for x in top_n.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest('apple', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might look not so promising. Remember about the upgrades to word2vec: subsampling and negative sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
